{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXLXulmI7Chq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "buzzfeed_df = pd.read_csv('data/buzzfeed.csv')\n",
        "isot_df = pd.read_csv('data/isot.csv')"
      ],
      "metadata": {
        "id": "Is1mT-rM7Hti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([buzzfeed_df, isot_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "2DlvdJrW7KoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df['label'] = combined_df['label'].map({'fake': 0, 'real': 1})"
      ],
      "metadata": {
        "id": "BZL57mQf7NhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer"
      ],
      "metadata": {
        "id": "zemEfwCZ7Ycp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
      ],
      "metadata": {
        "id": "UJ9Mt7jc7bP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    return tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n"
      ],
      "metadata": {
        "id": "i6sDBwzd7eHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_texts = combined_df['text'].apply(tokenize)"
      ],
      "metadata": {
        "id": "NAz1gz6V7hEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.stack([item['input_ids'].squeeze() for item in tokenized_texts])\n",
        "attention_masks = torch.stack([item['attention_mask'].squeeze() for item in tokenized_texts])\n",
        "labels = torch.tensor(combined_df['label'].values)"
      ],
      "metadata": {
        "id": "JUve_KWU7jpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import RobertaModel"
      ],
      "metadata": {
        "id": "93RgQqIe7mWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoBERTaForFakeNewsDetection(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RoBERTaForFakeNewsDetection, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.classifier = nn.Linear(self.roberta.config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
        "        return logits"
      ],
      "metadata": {
        "id": "-J0TKfnA7qu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AdamW"
      ],
      "metadata": {
        "id": "SVBsur8D7t3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "SBcg-UWl7yU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RoBERTaForFakeNewsDetection()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=1e-2)"
      ],
      "metadata": {
        "id": "fJhlh1-x725n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "w1nPoj4o75sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-7jAiFI8AyJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}